"""
Common data structures and utilities.
"""

import ast
import dataclasses
import glob
import json
import os
import re
import time
from typing import Optional

import openai
import anthropic

from fastchat.model.model_adapter import (
    get_conversation_template,
    ANTHROPIC_MODEL_LIST,
    OPENAI_MODEL_LIST,
)

# API setting constants
API_MAX_RETRY = 32
API_RETRY_SLEEP = 10
API_ERROR_OUTPUT = "$ERROR$"

TIE_DELTA = 0.1

# Categories that need reference answers
NEED_REF_CATS = ["math", "reasoning", "coding", "arena-hard-200"]

# Extract scores from judgments
two_score_pattern = re.compile("\[\[(\d+\.?\d*),\s?(\d+\.?\d*)\]\]")
two_score_pattern_backup = re.compile("\[(\d+\.?\d*),\s?(\d+\.?\d*)\]")
one_score_pattern = re.compile("\[\[(\d+\.?\d*)\]\]")
one_score_pattern_backup = re.compile("\[(\d+\.?\d*)\]")

# Sampling temperature configs for
temperature_config = {
    "writing": 0.7,
    "roleplay": 0.7,
    "extraction": 0.0,
    "math": 0.0,
    "coding": 0.0,
    "reasoning": 0.0,
    "stem": 0.1,
    "humanities": 0.1,
    "arena-hard-200": 0.0,
}

reverse_model_map = {
    "model_1": "model_2",
    "model_2": "model_1",
}


@dataclasses.dataclass
class Judge:
    """
    Represents the configuration for a judging process, including model, prompt template, 
    and flags for reference-based or multi-turn evaluation.
    
    Attributes:
        model_name (str): Name of the model used for judgment.
        prompt_template (dict): Template used to prompt the judge.
        ref_based (bool): Indicates if the judgment is based on a reference answer.
        multi_turn (bool): Indicates if the evaluation involves multiple turns (multi-turn dialogue).
    """
    model_name: str
    prompt_template: dict
    ref_based: bool = False
    multi_turn: bool = False


@dataclasses.dataclass
class MatchSingle:
    """
    Represents a single-model match instance for evaluation.
    
    Attributes:
        question (dict): The question to be evaluated.
        model (str): Name of the model that produced the answer.
        answer (dict): The answer generated by the model.
        judge (Judge): Judge configuration used for the evaluation.
        ref_answer (dict, optional): Reference (ground-truth) answer for judgment.
        multi_turn (bool): Indicates if the instance represents a multi-turn dialogue.
    """
    question: dict
    model: str
    answer: dict
    judge: Judge
    ref_answer: dict = None
    multi_turn: bool = False


@dataclasses.dataclass
class MatchPair:
    """
    Represents a pairwise match instance, comparing answers from two models.
    
    Attributes:
        question (dict): The question to be evaluated.
        model_1 (str): Name of the first model.
        model_2 (str): Name of the second model.
        answer_1 (dict): The answer generated by the first model.
        answer_2 (dict): The answer generated by the second model.
        judge (Judge): Judge configuration used for the evaluation.
        ref_answer (dict, optional): Reference (ground-truth) answer for judgment.
        multi_turn (bool): Indicates if the instance represents a multi-turn dialogue.
    """
    question: dict
    model_1: str
    model_2: str
    answer_1: dict
    answer_2: dict
    judge: Judge
    ref_answer: dict = None
    multi_turn: bool = False


def load_questions(question_file: str, begin: Optional[int], end: Optional[int]):
    """
    Loads questions from a file, returning the selected range.
    
    Args:
        question_file (str): Path to the file containing questions
         (one per line, in JSON format).
        begin (int or None): Start index for selection (inclusive).
        end (int or None): End index for selection (exclusive).

    Returns:
        list[dict]: List of question dictionaries within the specified range.
    """
    questions = []
    with open(question_file, "r") as ques_file:
        for line in ques_file:
            if line:
                questions.append(json.loads(line))
    questions = questions[begin:end]
    return questions

def read2json(filename):
    """
    Reads a JSON file and returns the parsed object.
    
    Args:
        filename (str): Path to the JSON file.

    Returns:
        Any: Parsed Python object from the JSON file.
    """
    with open(filename, 'r', encoding='utf-8') as fr:
        return json.load(fr)

def load_model_answers(answer_dir: str):
    """Load model answers.

    The return value is a python dict of type:
    Dict[model_name: str -> Dict[question_id: int -> answer: dict]]
    """
    filenames = glob.glob(os.path.join(answer_dir, "*.jsonl"))
    filenames.sort()
    model_answers = {}

    for filename in filenames:
        model_name = os.path.basename(filename)[:-6]
        answer = {}
        print(f'Proccesing: {filename}')
        try:
            with open(filename) as fin:
                for line in fin:
                    line = json.loads(line)
                    answer[line["question_id"]] = line
        except Exception as e:
            answer = {}
            for line in read2json(filename):
                answer[line["question_id"]] = line

        model_answers[model_name] = answer

    return model_answers


def load_judge_prompts(prompt_file: str):
    """Load judge prompts.

    The return value is a python dict of type:
    Dict[judge_name: str -> dict]
    """
    prompts = {}
    with open(prompt_file) as fin:
        for line in fin:
            line = json.loads(line)
            prompts[line["name"]] = line
    return prompts


def run_judge_single(question, answer, judge, ref_answer, multi_turn=False):
    """
    Evaluates a single answer to a question using the provided judge model and prompt template.

    Args:
        question (dict): The question to be evaluated, structured as a dict with
         'turns' and 'question_id'.
        answer (dict): The model's answer, structured with 'choices', each having 'turns'.
        judge (object): The judge model object, which includes
         'model_name' and 'prompt_template' info.
        ref_answer (dict or None): Optional reference answer, with a similar structure as answer.
        multi_turn (bool): Set True if the input contains multiple turns (dialogue), else False.

    Returns:
        tuple:
            - rating (int): The score assigned by the judge, -1 if judgment failed or invalid.
            - user_prompt (str): The prompt presented to the judge model.
            - judgment (str): The raw output returned by the judge model.

    Raises:
        ValueError: If the judge's model name or output format is invalid.
    """
    kwargs = {}
    model = judge.model_name
    if ref_answer is not None:
        kwargs["ref_answer_1"] = ref_answer["choices"][0]["turns"][0]
        if multi_turn:
            kwargs["ref_answer_2"] = ref_answer["choices"][0]["turns"][1]

    if multi_turn:
        user_prompt = judge.prompt_template["prompt_template"].format(
            question_1=question["turns"][0],
            question_2=question["turns"][1],
            answer_1=answer["choices"][0]["turns"][0],
            answer_2=answer["choices"][0]["turns"][1],
            **kwargs,
        )
    else:
        user_prompt = judge.prompt_template["prompt_template"].format(
            question=question["turns"][0],
            answer=answer["choices"][0]["turns"][0],
            **kwargs,
        )

    rating = -1

    system_prompt = judge.prompt_template["system_prompt"]
    conv = get_conversation_template(model)
    conv.set_system_message(system_prompt)
    conv.append_message(conv.roles[0], user_prompt)
    conv.append_message(conv.roles[1], None)

    if model in OPENAI_MODEL_LIST:
        judgment = chat_completion_openai(model, conv, temperature=0, max_tokens=2048)
    elif model in ANTHROPIC_MODEL_LIST:
        judgment = chat_completion_anthropic(
            model, conv, temperature=0, max_tokens=1024
        )
    else:
        raise ValueError(f"Invalid judge model name: {model}")

    if judge.prompt_template["output_format"] == "[[rating]]":
        match = re.search(one_score_pattern, judgment)
        if not match:
            match = re.search(one_score_pattern_backup, judgment)

        if match:
            rating = ast.literal_eval(match.groups()[0])
        else:
            rating = -1
    else:
        raise ValueError(
            f"invalid output format: {judge.prompt_template['output_format']}"
        )

    return rating, user_prompt, judgment


def play_a_match_single(match: MatchSingle, output_file: str):
    """
    Plays out a single QA-Evaluation match, using the provided judge and model, 
        and stores the result.

    Args:
        match (MatchSingle): An object containing details of the single match, including
              question, answer, judge, etc.
        output_file (str): Path to the output file where the result is to be saved.

    Returns:
        dict: A dictionary containing details about the match, including 
              question_id, model, judge, user_prompt, 
              judgment, score, turn, and timestamp.
        None: If the match result is not valid for output (score == -1).

    Raises:
        ValueError: If the judge type is not "single".
    """
    question, model, answer, judge, ref_answer, multi_turn = (
        match.question,
        match.model,
        match.answer,
        match.judge,
        match.ref_answer,
        match.multi_turn,
    )

    if judge.prompt_template["type"] == "single":
        score, user_prompt, judgment = run_judge_single(
            question, answer, judge, ref_answer, multi_turn=multi_turn
        )

        question_id = question["question_id"]
        turn = 1 if not multi_turn else 2
        result = {
            "question_id": question_id,
            "model": model,
            "judge": (judge.model_name, judge.prompt_template["name"]),
            "user_prompt": user_prompt,
            "judgment": judgment,
            "score": score,
            "turn": turn,
            "tstamp": time.time(),
        }
        print(
            f"question: {question_id}, turn: {turn}, model: {model}, "
            f"score: {score}, "
            f"judge: {(judge.model_name, judge.prompt_template['name'])}"
        )
    else:
        raise ValueError(f"invalid judge type: {judge['type']}")

    if output_file and result['score']!=-1:
        # os.makedirs(os.path.dirname(output_file), exist_ok=True)
        # with open(output_file, "a") as fout:
        #     fout.write(json.dumps(result) + "\n")
        pass
    else:
        print(f'Skip writing for {output_file} with score={result["score"]}')
        return None

    return result


import logging
import requests
import sys
sys.path.append('..')
from openai_api import chat_completions

def qwen3_eval_result(model,messages,n,temperature,max_tokens):
    return chat_completions(messages, n=n, temperature=temperature,max_new_tokens=8024)

def chat_completion_openai(model, conv, temperature, max_tokens, api_dict=None):
    if api_dict is not None:
        openai.api_base = api_dict["api_base"]
        openai.api_key = api_dict["api_key"]
    output = API_ERROR_OUTPUT
    for _ in range(API_MAX_RETRY):
        try:
            messages = conv.to_openai_api_messages()
            response = qwen3_eval_result(
                model=model,
                messages=messages,
                n=1,
                temperature=temperature,
                max_tokens=max_tokens,
            )
            output = response[0]
            break
        except Exception as e:
            print(type(e), e)
            time.sleep(API_RETRY_SLEEP)

    return output

def chat_completion_anthropic(model, conv, temperature, max_tokens, api_dict=None):
    if api_dict is not None and "api_key" in api_dict:
        api_key = api_dict["api_key"]
    else:
        api_key = os.environ["ANTHROPIC_API_KEY"]

    output = API_ERROR_OUTPUT
    for _ in range(API_MAX_RETRY):
        try:
            c = anthropic.Anthropic(api_key=api_key)
            prompt = conv.get_prompt()
            response = c.completions.create(
                model=model,
                prompt=prompt,
                stop_sequences=[anthropic.HUMAN_PROMPT],
                max_tokens_to_sample=max_tokens,
                temperature=temperature,
            )
            output = response.completion
            break
        except anthropic.APIError as e:
            print(type(e), e)
            time.sleep(API_RETRY_SLEEP)
    return output.strip()


def load_single_model_judgments(filename: str):
    """Load model judgments.

    The return value is a dict of type:
    Dict[judge: Tuple -> Dict[game_key: tuple -> game_result: dict]
    """
    judge_dict = {}

    for line in open(filename):
        obj = json.loads(line)
        judge = tuple(obj["judge"])
        qid, model = obj["question_id"], obj["model"]

        if judge not in judge_dict:
            judge_dict[judge] = {}

        gamekey = (qid, model)

        judge_dict[judge][gamekey] = {
            "score": obj["score"],
            "judgment": obj["judgment"],
        }
    return judge_dict



def resolve_single_judgment_dict(
    question, model_judgments_normal, model_judgments_math, multi_turn=False
):
    """Return the correct single answer grading judge."""
    if multi_turn:
        if question["category"] in NEED_REF_CATS:
            return model_judgments_math[("gpt-4", "single-math-v1-multi-turn")]
        return model_judgments_normal[("gpt-4", "single-v1-multi-turn")]

    if question["category"] in NEED_REF_CATS:
        return model_judgments_math[("gpt-4", "single-math-v1")]
    else:
        return model_judgments_normal[("gpt-4", "single-v1")]


def get_single_judge_explanation(gamekey, judgment_dict):
    """Get model judge explanation."""
    try:
        qid, model = gamekey

        res = judgment_dict[gamekey]

        g1_judgment = res["judgment"]
        g1_score = res["score"]

        return (
            f"**Game 1**. **A**: {model}, **Score**: {g1_score}\n\n"
            f"**Judgment**: {g1_judgment}"
        )
    except KeyError:
        return "N/A"

def check_data(questions, model_answers, ref_answers, models, judges):
    """Check data completeness and return valid models."""
    valid_models = []
    
    for model in models:
        if _is_valid_model(model, model_answers, questions):
            valid_models.append(model)
    
    _check_reference_answers(judges, questions, ref_answers)
    
    return valid_models


def _is_valid_model(model, model_answers, questions):
    """Check if a model has complete answers for all questions."""
    if model not in model_answers:
        print(f"Warning: Missing model answer for {model}, skipping this model")
        return False
    
    return _has_all_answers(model, model_answers[model], questions)


def _has_all_answers(model, model_answer, questions):
    """Verify that the model has answered all questions."""
    for question in questions:
        if question["question_id"] not in model_answer:
            print(f"Warning: Missing model {model}'s "
                f"answer to Question {question['question_id']}, skipping this model")
            return False
    return True


def _check_reference_answers(judges, questions, ref_answers):
    """Check if reference-based judges have all required reference answers."""
    for judge in judges.values():
        if not judge.ref_based:
            continue
        
        _validate_judge_references(judge, questions, ref_answers)


def _validate_judge_references(judge, questions, ref_answers):
    """Validate reference answers for a specific judge."""
    for question in questions:
        if _needs_reference_check(question, judge, ref_answers):
            print(f"Warning: Missing reference answer to Question "
                f"{question['question_id']} for judge {judge.model_name}")


def _needs_reference_check(question, judge, ref_answers):
    """Check if a question needs reference validation for a judge."""
    return (question["category"] in NEED_REF_CATS and 
            question["question_id"] not in ref_answers[judge.model_name])


def get_model_list(answer_dir):
    file_paths = glob.glob(f"{answer_dir}/*.jsonl")
    file_names = [os.path.splitext(os.path.basename(f))[0] for f in file_paths]
    return file_names
